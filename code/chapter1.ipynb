{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 理解大语言模型 - Large Language Model (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> 主要结构如下：\n",
    "从raw data中进行预训练，得出基础模型（这一部分可以了解一下元学习的概念），这个基础模型所拥有的基础能力为文本补全、短时任务的推理能力。</br>\n",
    "> 在基础模型之上，可以导入自己标记的数据进行训练，这一部分可以成为微调（finetune），得到自己的LLM，可以用于分类，总结，翻译，个人助理等任务。\n",
    "\n",
    "![1716275709784](../image/从零开始构建LLM/1716275709784.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Transformer** 结构概览</br>\n",
    "1、输入需要被翻译的文本</br>\n",
    "2、预处理文本</br>\n",
    "3、编码器将输入文本进行编码</br>\n",
    "4、将编码部分送入解码器</br>\n",
    "5、模型每次只完成一个单词的翻译</br>\n",
    "6、预处理文本</br>\n",
    "7、解码器生成一个单词</br>\n",
    "8、完成翻译</br>\n",
    "\n",
    "![1716275687724](../image/从零开始构建LLM/1716275687724.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> BERT与GPT区别：BERT更多的使用于文本填空，GPT则是预测下一个单词。\n",
    "\n",
    "![1716275758151](../image/从零开始构建LLM/1716275758151.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **构建大模型步骤**</br>\n",
    "\n",
    "|阶段|子项|\n",
    "|---|---|\n",
    "|一|准备数据和样本|\n",
    "||实现注意力机制|\n",
    "||实现LLM结构|\n",
    "|二|训练|\n",
    "||模型评估|\n",
    "||加载预训练模型权重|\n",
    "|三|微调自己的模型|\n",
    "\n",
    "![1716275818354](../image/从零开始构建LLM/1716275818354.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
